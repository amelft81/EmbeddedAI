{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/EmbeddedAI/blob/main/Python_Code_for_Model_Pruning_(Revised_with_Specific_Architecture_Load).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Ensure this path correctly points to your 'simple_embedded_model.h5' file.\n",
        "# If the file is in the same directory as this script, just the filename is fine.\n",
        "# Otherwise, provide the full path, e.g., '/path/to/your/simple_embedded_model.h5'\n",
        "# Based on your latest output, it seems the file is now at /content/simple_embedded_model.h5\n",
        "# If you are running this in a Colab-like environment where files are uploaded to /content/,\n",
        "# then this path might be correct for that environment.\n",
        "SIMPLE_MODEL_PATH = '/content/simple_embedded_model.h5' # Updated based on your latest output\n",
        "\n",
        "# Output directory for optimized models\n",
        "OUTPUT_DIR = 'optimized_models'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def prune_and_save_model(model_path, target_sparsity=0.75, epochs=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Loads a Keras model, applies pruning, retrains it, and saves the pruned TFLite model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the original .h5 model file.\n",
        "        target_sparsity (float): The final target sparsity (e.g., 0.75 for 75% sparse weights).\n",
        "                                 Higher values aim for smaller models.\n",
        "        epochs (int): Number of epochs to retrain the pruned model. More epochs might be needed\n",
        "                      for higher sparsity to recover accuracy.\n",
        "        batch_size (int): Batch size for retraining.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting Pruning for {os.path.basename(model_path)} ---\")\n",
        "\n",
        "    # 1. Load the original model\n",
        "    model = None\n",
        "    try:\n",
        "        # Try loading the model directly\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Original model '{os.path.basename(model_path)}' loaded successfully.\")\n",
        "        model.summary()\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load model from '{model_path}'.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        print(\"Attempting to load with a specific architecture for simple_embedded_model.h5...\")\n",
        "        try:\n",
        "            # Based on the error \"Model expects 2 layers but the loaded weights have 4 layers.\"\n",
        "            # and common simple models, let's define a 4-layer Sequential model (Input + 3 Dense/Conv)\n",
        "            # This is a strong assumption about your model's structure.\n",
        "            # If this still fails, you MUST provide the exact model architecture.\n",
        "\n",
        "            # We need to infer the input shape first. From previous errors, it was (None, 10).\n",
        "            input_shape_for_dummy = (10,) # Default, adjust if your model's input is different\n",
        "\n",
        "            # Attempt to infer input shape from the error message again, if it's there\n",
        "            import re\n",
        "            match = re.search(r\"'batch_shape': \\[None, (\\d+)\\]\", str(e))\n",
        "            if match:\n",
        "                input_shape_for_dummy = (int(match.group(1)),)\n",
        "                print(f\"Inferred input shape from error: {input_shape_for_dummy}\")\n",
        "            else:\n",
        "                print(f\"Could not infer input shape from error. Using default: {input_shape_for_dummy}\")\n",
        "\n",
        "            # Define a 4-layer sequential model (Input + 3 trainable layers)\n",
        "            # This is a common structure for simple_embedded_model.h5 based on uTensorEdgeImpulse.ipynb\n",
        "            # from your uploaded files, which shows:\n",
        "            # InputLayer(input_shape=(10,)), Dense(8, activation='relu'), Dense(1, activation='sigmoid')\n",
        "            # This is 3 layers, plus the InputLayer makes 4.\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.InputLayer(input_shape=input_shape_for_dummy),\n",
        "                tf.keras.layers.Dense(8, activation='relu'), # Assuming 8 units based on uTensorEdgeImpulse.ipynb\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid') # Assuming 1 output unit, sigmoid for binary classification\n",
        "            ])\n",
        "\n",
        "            # Load weights into this newly defined model\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"Model architecture defined and weights loaded from '{os.path.basename(model_path)}'. Model summary:\")\n",
        "            model.summary()\n",
        "\n",
        "        except Exception as load_weights_e:\n",
        "            print(f\"CRITICAL ERROR: Failed to load model even with specific architecture attempt: {load_weights_e}\")\n",
        "            print(\"This indicates the assumed architecture might be incorrect or another issue.\")\n",
        "            print(\"Please provide the exact Keras architecture of your 'simple_embedded_model.h5' if this persists.\")\n",
        "            print(\"Exiting pruning process for this model.\")\n",
        "            return\n",
        "\n",
        "    # 2. Prepare Dummy Data for Demonstration (REPLACE THIS WITH YOUR ACTUAL TRAINING DATA)\n",
        "    # IMPORTANT: Determine the correct input shape for your model.\n",
        "    # You can inspect `model.input_shape` after loading the model to confirm.\n",
        "    input_shape_for_dummy = model.input_shape[1:] # Get input shape excluding batch dimension\n",
        "    num_samples = 1000\n",
        "\n",
        "    # Determine number of classes/output shape for y_train and loss function\n",
        "    if len(model.output_shape) > 1:\n",
        "        num_classes = model.output_shape[-1]\n",
        "    else:\n",
        "        num_classes = 1\n",
        "\n",
        "    X_train = np.random.rand(num_samples, *input_shape_for_dummy).astype(np.float32)\n",
        "\n",
        "    if num_classes > 1:\n",
        "        y_train = np.random.randint(0, num_classes, num_samples)\n",
        "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    elif num_classes == 1:\n",
        "        if hasattr(model.layers[-1], 'activation') and model.layers[-1].activation == tf.keras.activations.sigmoid:\n",
        "            y_train = np.random.randint(0, 2, num_samples).astype(np.float32)\n",
        "            loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "        else:\n",
        "            y_train = np.random.rand(num_samples, num_classes).astype(np.float32)\n",
        "            loss_function = tf.keras.losses.MeanSquaredError()\n",
        "    else:\n",
        "        y_train = np.random.rand(num_samples, num_classes).astype(np.float32)\n",
        "        loss_function = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "\n",
        "    print(f\"Dummy training data created with shape X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Define the pruning schedule\n",
        "    end_step = len(X_train) // batch_size * epochs\n",
        "\n",
        "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.0,\n",
        "        final_sparsity=target_sparsity,\n",
        "        begin_step=0,\n",
        "        end_step=end_step\n",
        "    )\n",
        "\n",
        "    # 4. Apply pruning to the model\n",
        "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
        "\n",
        "    # 5. Recompile the pruned model (important!)\n",
        "    pruned_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=loss_function,\n",
        "        metrics=['accuracy'] if num_classes > 1 or (num_classes == 1 and hasattr(model.layers[-1], 'activation') and model.layers[-1].activation == tf.keras.activations.sigmoid) else ['mse']\n",
        "    )\n",
        "    print(\"Pruned model compiled.\")\n",
        "    pruned_model.summary()\n",
        "\n",
        "    # 6. Train the pruned model (fine-tuning)\n",
        "    print(f\"\\nTraining pruned model with target sparsity {target_sparsity*100:.0f}% over {epochs} epochs...\")\n",
        "    pruned_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[tfmot.sparsity.keras.UpdatePruningStep()],\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"Pruned model training complete.\")\n",
        "\n",
        "    # 7. Strip the pruning wrappers\n",
        "    model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "    print(\"Pruning wrappers stripped.\")\n",
        "\n",
        "    # 8. Convert to TFLite model\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # 9. Save the pruned TFLite model\n",
        "    pruned_tflite_path = os.path.join(OUTPUT_DIR, f'pruned_model_sparsity_{int(target_sparsity*100)}.tflite')\n",
        "    with open(pruned_tflite_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    pruned_tflite_size_kb = os.path.getsize(pruned_tflite_path) / 1024\n",
        "    print(f\"Pruned TFLite model saved to: {pruned_tflite_path}\")\n",
        "    print(f\"Pruned TFLite model size: {pruned_tflite_size_kb:.2f} KB\")\n",
        "\n",
        "    # Compare with the target size\n",
        "    target_size_to_beat_kb = 15407.12\n",
        "    if pruned_tflite_size_kb < target_size_to_beat_kb:\n",
        "        print(f\"SUCCESS: Pruned model size ({pruned_tflite_size_kb:.2f} KB) is LESS than the previous pruned size ({target_size_to_beat_kb:.2f} KB).\")\n",
        "    else:\n",
        "        print(f\"NOTE: Pruned model size ({pruned_tflite_size_kb:.2f} KB) is NOT less than the previous pruned size ({target_size_to_beat_kb:.2f} KB).\")\n",
        "        print(\"Consider increasing 'target_sparsity' or 'epochs' for more aggressive pruning.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Call the function to prune your simple_embedded_model.h5\n",
        "    # Start with a higher target_sparsity (e.g., 0.8 or 0.9) to aim for a smaller model.\n",
        "    # You might also need to increase the number of epochs.\n",
        "    prune_and_save_model(SIMPLE_MODEL_PATH, target_sparsity=0.85, epochs=20)\n",
        "\n",
        "    # If you want to try pruning MobileNetV2 (this will take much longer and require more resources):\n",
        "    # MOBILENET_V2_MODEL_PATH = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5'\n",
        "    # prune_and_save_model(MOBILENET_V2_MODEL_PATH, target_sparsity=0.5, epochs=5) # Start lower sparsity for large models"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Pruning for simple_embedded_model.h5 ---\n",
            "ERROR: Could not load model from '/content/simple_embedded_model.h5'.\n",
            "Details: Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 10], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n",
            "\n",
            "Exception encountered: Unrecognized keyword arguments: ['batch_shape']\n",
            "Attempting to load with a specific architecture for simple_embedded_model.h5...\n",
            "Inferred input shape from error: (10,)\n",
            "Model architecture defined and weights loaded from 'simple_embedded_model.h5'. Model summary:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8)                 88        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97 (388.00 Byte)\n",
            "Trainable params: 97 (388.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Dummy training data created with shape X_train: (1000, 10), y_train: (1000,)\n",
            "Pruned model compiled.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_dense   (None, 8)                 170       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 1)                 19        \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 189 (764.00 Byte)\n",
            "Trainable params: 97 (388.00 Byte)\n",
            "Non-trainable params: 92 (376.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Training pruned model with target sparsity 85% over 20 epochs...\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 2ms/step - loss: 0.6977 - accuracy: 0.5110\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6969 - accuracy: 0.5120\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.5130\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.5080\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6961 - accuracy: 0.5170\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.5110\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7055 - accuracy: 0.4880\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6983 - accuracy: 0.4940\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5310\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6958 - accuracy: 0.5180\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.5270\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6939 - accuracy: 0.5270\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5270\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6945 - accuracy: 0.5270\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5270\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5030\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.4730\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4730\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4990\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5270\n",
            "Pruned model training complete.\n",
            "Pruning wrappers stripped.\n",
            "Pruned TFLite model saved to: optimized_models/pruned_model_sparsity_85.tflite\n",
            "Pruned TFLite model size: 1.99 KB\n",
            "SUCCESS: Pruned model size (1.99 KB) is LESS than the previous pruned size (15407.12 KB).\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7kuWWiv46vY",
        "outputId": "d6eaebb1-df7f-4608-871e-6d7a277c00bd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "e009a8a4",
        "outputId": "5eb282f1-e2d9-4b23-fa0a-2c36627cc130"
      },
      "source": [
        "!pip install tensorflow-model-optimization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Collecting numpy~=1.23 (from tensorflow-model-optimization)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorflow-model-optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tensorflow-model-optimization-0.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "502ad482fb454dc8a8d79e07b7e4f5d4"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}