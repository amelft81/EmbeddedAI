{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/EmbeddedAI/blob/main/Model_Quantization_and_Pruning_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your .h5 model files\n",
        "MOBILENET_V2_MODEL_PATH = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5'\n",
        "SIMPLE_MODEL_PATH = 'simple_embedded_model.h5' # This model will be used for pruning/QAT illustration\n",
        "\n",
        "# Output directory for optimized models\n",
        "OUTPUT_DIR = 'optimized_models'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define a representative dataset for full integer quantization\n",
        "# This should be a generator or a list of your actual input data\n",
        "# For demonstration, we'll use dummy data.\n",
        "def representative_dataset_gen():\n",
        "    \"\"\"\n",
        "    A generator function for a representative dataset.\n",
        "    This is crucial for full integer quantization.\n",
        "    Replace this with your actual data loading logic.\n",
        "    \"\"\"\n",
        "    # Assuming input shape for MobileNetV2 is (224, 224, 3)\n",
        "    for _ in range(100): # Generate 100 samples\n",
        "        data = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
        "        yield [data]\n",
        "\n",
        "# --- Helper function to get model size ---\n",
        "def get_gzipped_model_size(file_path):\n",
        "    \"\"\"\n",
        "    Returns the size of a gzipped model in bytes.\n",
        "    \"\"\"\n",
        "    import zipfile\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "        f.write(file_path, os.path.basename(file_path))\n",
        "    return os.path.getsize(zipped_file)\n",
        "\n",
        "# --- 1. Load the Baseline Model ---\n",
        "print(f\"Loading baseline model from: {MOBILENET_V2_MODEL_PATH}\")\n",
        "try:\n",
        "    baseline_model = tf.keras.models.load_model(MOBILENET_V2_MODEL_PATH)\n",
        "    baseline_model.summary()\n",
        "    baseline_model_size = os.path.getsize(MOBILENET_V2_MODEL_PATH)\n",
        "    print(f\"Baseline model size (raw): {baseline_model_size / 1024:.2f} KB\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading baseline MobileNetV2 model: {e}\")\n",
        "    # If MobileNetV2 fails to load, create a dummy model for demonstration\n",
        "    print(\"Creating a simple dummy model for further demonstrations.\")\n",
        "    baseline_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    baseline_model.save(f\"{OUTPUT_DIR}/dummy_baseline_model.h5\")\n",
        "    baseline_model_size = os.path.getsize(f\"{OUTPUT_DIR}/dummy_baseline_model.h5\")\n",
        "    print(f\"Dummy Baseline model size (raw): {baseline_model_size / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "# --- 2. Post-Training Quantization ---\n",
        "\n",
        "print(\"\\n--- Applying Post-Training Dynamic Range Quantization ---\")\n",
        "converter_dr = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n",
        "converter_dr.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_dr = converter_dr.convert()\n",
        "\n",
        "dr_model_path = os.path.join(OUTPUT_DIR, 'quantized_dynamic_range_model.tflite')\n",
        "with open(dr_model_path, 'wb') as f:\n",
        "    f.write(tflite_model_dr)\n",
        "\n",
        "dr_model_size = os.path.getsize(dr_model_path)\n",
        "print(f\"Dynamic Range Quantized model saved to: {dr_model_path}\")\n",
        "print(f\"Dynamic Range Quantized model size (raw): {dr_model_size / 1024:.2f} KB\")\n",
        "# print(f\"Dynamic Range Quantized model size (gzipped): {get_gzipped_model_size(dr_model_path) / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Applying Post-Training Full Integer Quantization ---\")\n",
        "converter_int = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n",
        "converter_int.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_int.representative_dataset = representative_dataset_gen\n",
        "# Ensure all operations are quantized to integers. Fallback to float if not possible.\n",
        "converter_int.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Require full integer quantization, otherwise throw an error\n",
        "converter_int.inference_input_type = tf.float32 # Input type to the model\n",
        "converter_int.inference_output_type = tf.float32 # Output type from the model\n",
        "\n",
        "try:\n",
        "    tflite_model_int = converter_int.convert()\n",
        "    int_model_path = os.path.join(OUTPUT_DIR, 'quantized_full_integer_model.tflite')\n",
        "    with open(int_model_path, 'wb') as f:\n",
        "        f.write(tflite_model_int)\n",
        "\n",
        "    int_model_size = os.path.getsize(int_model_path)\n",
        "    print(f\"Full Integer Quantized model saved to: {int_model_path}\")\n",
        "    print(f\"Full Integer Quantized model size (raw): {int_model_size / 1024:.2f} KB\")\n",
        "    # print(f\"Full Integer Quantized model size (gzipped): {get_gzipped_model_size(int_model_path) / 1024:.2f} KB\")\n",
        "except Exception as e:\n",
        "    print(f\"Full Integer Quantization failed. This might happen if the model operations are not fully supported for INT8. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- 3. Pruning (Requires retraining) ---\n",
        "\n",
        "print(\"\\n--- Applying Pruning (Demonstration with a simple model) ---\")\n",
        "# For pruning, we'll create a simple model and train it with pruning.\n",
        "# This assumes you have a training dataset and validation dataset.\n",
        "pruning_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define pruning parameters\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.50,\n",
        "        final_sparsity=0.80,\n",
        "        begin_step=0,\n",
        "        end_step=1000 # Adjust based on your training steps/epochs\n",
        "    )\n",
        "}\n",
        "\n",
        "# Apply pruning to the model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(pruning_model, **pruning_params)\n",
        "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "pruned_model.summary()\n",
        "\n",
        "# Dummy data for training demonstration\n",
        "train_images = np.random.rand(100, 224, 224, 3).astype(np.float32)\n",
        "train_labels = np.random.randint(0, 10, 100)\n",
        "test_images = np.random.rand(20, 224, 224, 3).astype(np.float32)\n",
        "test_labels = np.random.randint(0, 10, 20)\n",
        "\n",
        "print(\"Training pruned model (using dummy data)...\")\n",
        "# You would replace this with your actual training data and more epochs\n",
        "pruned_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=2, # Keep low for demonstration\n",
        "    validation_data=(test_images, test_labels),\n",
        "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()]\n",
        ")\n",
        "\n",
        "# Strip pruning wrappers for inference\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "pruned_keras_path = os.path.join(OUTPUT_DIR, 'pruned_keras_model.h5')\n",
        "model_for_export.save(pruned_keras_path, include_optimizer=False)\n",
        "print(f\"Pruned Keras model saved to: {pruned_keras_path}\")\n",
        "pruned_keras_size = os.path.getsize(pruned_keras_path)\n",
        "print(f\"Pruned Keras model size (raw): {pruned_keras_size / 1024:.2f} KB\")\n",
        "\n",
        "# Convert pruned Keras model to TFLite\n",
        "converter_pruned = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "tflite_pruned_model = converter_pruned.convert()\n",
        "\n",
        "pruned_tflite_path = os.path.join(OUTPUT_DIR, 'pruned_tflite_model.tflite')\n",
        "with open(pruned_tflite_path, 'wb') as f:\n",
        "    f.write(tflite_pruned_model)\n",
        "\n",
        "pruned_tflite_size = os.path.getsize(pruned_tflite_path)\n",
        "print(f\"Pruned TFLite model saved to: {pruned_tflite_path}\")\n",
        "print(f\"Pruned TFLite model size (raw): {pruned_tflite_size / 1024:.2f} KB\")\n",
        "# print(f\"Pruned TFLite model size (gzipped): {get_gzipped_model_size(pruned_tflite_path) / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "# --- 4. Quantization-Aware Training (QAT) ---\n",
        "\n",
        "print(\"\\n--- Applying Quantization-Aware Training (Demonstration with a simple model) ---\")\n",
        "# For QAT, we'll use a simple model and train it with QAT applied.\n",
        "qat_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Apply quantization-aware training wrappers\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(qat_model)\n",
        "quant_aware_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "quant_aware_model.summary()\n",
        "\n",
        "print(\"Training quantization-aware model (using dummy data)...\")\n",
        "# You would replace this with your actual training data and more epochs\n",
        "quant_aware_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=2, # Keep low for demonstration\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n",
        "# Convert QAT model to TFLite\n",
        "converter_qat = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter_qat.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_qat_model = converter_qat.convert()\n",
        "\n",
        "qat_tflite_path = os.path.join(OUTPUT_DIR, 'qat_tflite_model.tflite')\n",
        "with open(qat_tflite_path, 'wb') as f:\n",
        "    f.write(tflite_qat_model)\n",
        "\n",
        "qat_tflite_size = os.path.getsize(qat_tflite_path)\n",
        "print(f\"Quantization-Aware Trained TFLite model saved to: {qat_tflite_path}\")\n",
        "print(f\"Quantization-Aware Trained TFLite model size (raw): {qat_tflite_size / 1024:.2f} KB\")\n",
        "# print(f\"Quantization-Aware Trained TFLite model size (gzipped): {get_gzipped_model_size(qat_tflite_path) / 1024:.2f} KB\")\n",
        "\n",
        "print(\"\\n--- Summary of Model Sizes (Raw .tflite) ---\")\n",
        "print(f\"Baseline model (original .h5): {baseline_model_size / 1024:.2f} KB\")\n",
        "print(f\"Dynamic Range Quantized TFLite: {dr_model_size / 1024:.2f} KB\")\n",
        "if 'int_model_size' in locals():\n",
        "    print(f\"Full Integer Quantized TFLite: {int_model_size / 1024:.2f} KB\")\n",
        "print(f\"Pruned TFLite: {pruned_tflite_size / 1024:.2f} KB\")\n",
        "print(f\"Quantization-Aware Trained TFLite: {qat_tflite_size / 1024:.2f} KB\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading baseline model from: mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5\n",
            "Error loading baseline MobileNetV2 model: No file or directory found at mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5\n",
            "Creating a simple dummy model for further demonstrations.\n",
            "Dummy Baseline model size (raw): 15422.55 KB\n",
            "\n",
            "--- Applying Post-Training Dynamic Range Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Range Quantized model saved to: optimized_models/quantized_dynamic_range_model.tflite\n",
            "Dynamic Range Quantized model size (raw): 3856.03 KB\n",
            "\n",
            "--- Applying Post-Training Full Integer Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Integer Quantized model saved to: optimized_models/quantized_full_integer_model.tflite\n",
            "Full Integer Quantized model size (raw): 3854.87 KB\n",
            "\n",
            "--- Applying Pruning (Demonstration with a simple model) ---\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d  (None, 222, 222, 32)      1762      \n",
            " _7 (PruneLowMagnitude)                                          \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 111, 111, 32)      1         \n",
            " oling2d_7 (PruneLowMagnitu                                      \n",
            " de)                                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 394272)            1         \n",
            " n_7 (PruneLowMagnitude)                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 10)                7885452   \n",
            " 7 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7887216 (30.09 MB)\n",
            "Trainable params: 3943626 (15.04 MB)\n",
            "Non-trainable params: 3943590 (15.04 MB)\n",
            "_________________________________________________________________\n",
            "Training pruned model (using dummy data)...\n",
            "Epoch 1/2\n",
            "4/4 [==============================] - ETA: 0s - loss: 47.8551 - accuracy: 0.0600"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7b1998193240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 7s 1s/step - loss: 47.8551 - accuracy: 0.0600 - val_loss: 149.6461 - val_accuracy: 0.0500\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 4s 853ms/step - loss: 133.1924 - accuracy: 0.1800 - val_loss: 142.6413 - val_accuracy: 0.1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Keras model saved to: optimized_models/pruned_keras_model.h5\n",
            "Pruned Keras model size (raw): 15421.12 KB\n",
            "Pruned TFLite model saved to: optimized_models/pruned_tflite_model.tflite\n",
            "Pruned TFLite model size (raw): 15407.12 KB\n",
            "\n",
            "--- Applying Quantization-Aware Training (Demonstration with a simple model) ---\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer_2 (Quantize  (None, 224, 224, 3)       3         \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_8 (QuantizeWr  (None, 222, 222, 32)      963       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_max_pooling2d_8 (Qua  (None, 111, 111, 32)      1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_flatten_8 (QuantizeW  (None, 394272)            1         \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_dense_8 (QuantizeWra  (None, 10)                3942735   \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3943703 (15.04 MB)\n",
            "Trainable params: 3943626 (15.04 MB)\n",
            "Non-trainable params: 77 (308.00 Byte)\n",
            "_________________________________________________________________\n",
            "Training quantization-aware model (using dummy data)...\n",
            "Epoch 1/2\n",
            "4/4 [==============================] - ETA: 0s - loss: 4.5410 - accuracy: 0.0800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7b1998908d60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 7s 1s/step - loss: 4.5410 - accuracy: 0.0800 - val_loss: 5.7315 - val_accuracy: 0.0500\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 4s 1s/step - loss: 5.3108 - accuracy: 0.0800 - val_loss: 6.2337 - val_accuracy: 0.0500\n",
            "Quantization-Aware Trained TFLite model saved to: optimized_models/qat_tflite_model.tflite\n",
            "Quantization-Aware Trained TFLite model size (raw): 3855.23 KB\n",
            "\n",
            "--- Summary of Model Sizes (Raw .tflite) ---\n",
            "Baseline model (original .h5): 15422.55 KB\n",
            "Dynamic Range Quantized TFLite: 3856.03 KB\n",
            "Full Integer Quantized TFLite: 3854.87 KB\n",
            "Pruned TFLite: 15407.12 KB\n",
            "Quantization-Aware Trained TFLite: 3855.23 KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp_C0KDQo1ls",
        "outputId": "8e26ef0b-10eb-4e56-cc5d-d90f0f8bb26f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8b49232"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae8833b",
        "outputId": "1b723137-1c1d-4197-e1ed-2d55f1e1ba84"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot # Import after installation\n",
        "import os\n",
        "import numpy as np\n",
        "import tempfile # Added missing import\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your .h5 model files\n",
        "# Ensure these files exist or update the paths\n",
        "MOBILENET_V2_MODEL_PATH = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5'\n",
        "SIMPLE_MODEL_PATH = 'simple_embedded_model.h5' # This model will be used for pruning/QAT illustration\n",
        "\n",
        "# Output directory for optimized models\n",
        "OUTPUT_DIR = 'optimized_models'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define a representative dataset for full integer quantization\n",
        "# This should be a generator or a list of your actual input data\n",
        "# For demonstration, we'll use dummy data.\n",
        "def representative_dataset_gen():\n",
        "    \"\"\"\n",
        "    A generator function for a representative dataset.\n",
        "    This is crucial for full integer quantization.\n",
        "    Replace this with your actual data loading logic.\n",
        "    \"\"\"\n",
        "    # Assuming input shape for MobileNetV2 is (224, 224, 3)\n",
        "    for _ in range(100): # Generate 100 samples\n",
        "        data = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
        "        yield [data]\n",
        "\n",
        "# --- Helper function to get model size ---\n",
        "def get_gzipped_model_size(file_path):\n",
        "    \"\"\"\n",
        "    Returns the size of a gzipped model in bytes.\n",
        "    \"\"\"\n",
        "    import zipfile\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "        f.write(file_path, os.path.basename(file_path))\n",
        "    return os.path.getsize(zipped_file)\n",
        "\n",
        "# --- 1. Load the Baseline Model ---\n",
        "print(f\"Loading baseline model from: {MOBILENET_V2_MODEL_PATH}\")\n",
        "try:\n",
        "    baseline_model = tf.keras.models.load_model(MOBILENET_V2_MODEL_PATH)\n",
        "    baseline_model.summary()\n",
        "    baseline_model_size = os.path.getsize(MOBILENET_V2_MODEL_PATH)\n",
        "    print(f\"Baseline model size (raw): {baseline_model_size / 1024:.2f} KB\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading baseline MobileNetV2 model: {e}\")\n",
        "    # If MobileNetV2 fails to load, create a dummy model for demonstration\n",
        "    print(\"Creating a simple dummy model for further demonstrations.\")\n",
        "    baseline_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    # Ensure the dummy model is saved to the OUTPUT_DIR\n",
        "    dummy_model_path = os.path.join(OUTPUT_DIR, \"dummy_baseline_model.h5\")\n",
        "    baseline_model.save(dummy_model_path)\n",
        "    baseline_model_size = os.path.getsize(dummy_model_path)\n",
        "    print(f\"Dummy Baseline model size (raw): {baseline_model_size / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "# --- 2. Post-Training Quantization ---\n",
        "\n",
        "print(\"\\n--- Applying Post-Training Dynamic Range Quantization ---\")\n",
        "converter_dr = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n",
        "converter_dr.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_dr = converter_dr.convert()\n",
        "\n",
        "dr_model_path = os.path.join(OUTPUT_DIR, 'quantized_dynamic_range_model.tflite')\n",
        "with open(dr_model_path, 'wb') as f:\n",
        "    f.write(tflite_model_dr)\n",
        "\n",
        "dr_model_size = os.path.getsize(dr_model_path)\n",
        "print(f\"Dynamic Range Quantized model saved to: {dr_model_path}\")\n",
        "print(f\"Dynamic Range Quantized model size (raw): {dr_model_size / 1024:.2f} KB\")\n",
        "# print(f\"Dynamic Range Quantized model size (gzipped): {get_gzipped_model_size(dr_model_path) / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Applying Post-Training Full Integer Quantization ---\")\n",
        "converter_int = tf.lite.TFLiteConverter.from_keras_model(baseline_model)\n",
        "converter_int.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_int.representative_dataset = representative_dataset_gen\n",
        "# Ensure all operations are quantized to integers. Fallback to float if not possible.\n",
        "converter_int.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Require full integer quantization, otherwise throw an error\n",
        "# converter_int.inference_input_type = tf.float32 # This line is not needed and can cause issues\n",
        "# converter_int.inference_output_type = tf.float32 # This line is not needed and can cause issues\n",
        "\n",
        "try:\n",
        "    tflite_model_int = converter_int.convert()\n",
        "    int_model_path = os.path.join(OUTPUT_DIR, 'quantized_full_integer_model.tflite')\n",
        "    with open(int_model_path, 'wb') as f:\n",
        "        f.write(tflite_model_int)\n",
        "\n",
        "    int_model_size = os.path.getsize(int_model_path)\n",
        "    print(f\"Full Integer Quantized model saved to: {int_model_path}\")\n",
        "    print(f\"Full Integer Quantized model size (raw): {int_model_size / 1024:.2f} KB\")\n",
        "    # print(f\"Full Integer Quantized model size (gzipped): {get_gzipped_model_size(int_model_path) / 1024:.2f} KB\")\n",
        "except Exception as e:\n",
        "    print(f\"Full Integer Quantization failed. This might happen if the model operations are not fully supported for INT8. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- 3. Pruning (Requires retraining) ---\n",
        "\n",
        "print(\"\\n--- Applying Pruning (Demonstration with a simple model) ---\")\n",
        "# For pruning, we'll create a simple model and train it with pruning.\n",
        "# This assumes you have a training dataset and validation dataset.\n",
        "pruning_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define pruning parameters\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.50,\n",
        "        final_sparsity=0.80,\n",
        "        begin_step=0,\n",
        "        end_step=1000 # Adjust based on your training steps/epochs\n",
        "    )\n",
        "}\n",
        "\n",
        "# Apply pruning to the model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(pruning_model, **pruning_params)\n",
        "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "pruned_model.summary()\n",
        "\n",
        "# Dummy data for training demonstration\n",
        "train_images = np.random.rand(100, 224, 224, 3).astype(np.float32)\n",
        "train_labels = np.random.randint(0, 10, 100)\n",
        "test_images = np.random.rand(20, 224, 224, 3).astype(np.float32)\n",
        "test_labels = np.random.randint(0, 10, 20)\n",
        "\n",
        "print(\"Training pruned model (using dummy data)...\")\n",
        "# You would replace this with your actual training data and more epochs\n",
        "pruned_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=2, # Keep low for demonstration\n",
        "    validation_data=(test_images, test_labels),\n",
        "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()]\n",
        ")\n",
        "\n",
        "# Strip pruning wrappers for inference\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "pruned_keras_path = os.path.join(OUTPUT_DIR, 'pruned_keras_model.h5')\n",
        "model_for_export.save(pruned_keras_path, include_optimizer=False)\n",
        "print(f\"Pruned Keras model saved to: {pruned_keras_path}\")\n",
        "pruned_keras_size = os.path.getsize(pruned_keras_path)\n",
        "print(f\"Pruned Keras model size (raw): {pruned_keras_size / 1024:.2f} KB\")\n",
        "\n",
        "# Convert pruned Keras model to TFLite\n",
        "converter_pruned = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "tflite_pruned_model = converter_pruned.convert()\n",
        "\n",
        "pruned_tflite_path = os.path.join(OUTPUT_DIR, 'pruned_tflite_model.tflite')\n",
        "with open(pruned_tflite_path, 'wb') as f:\n",
        "    f.write(tflite_pruned_model)\n",
        "\n",
        "pruned_tflite_size = os.path.getsize(pruned_tflite_path)\n",
        "print(f\"Pruned TFLite model saved to: {pruned_tflite_path}\")\n",
        "print(f\"Pruned TFLite model size (raw): {pruned_tflite_size / 1024:.2f} KB\")\n",
        "# print(f\"Pruned TFLite model size (gzipped): {get_gzipped_model_size(pruned_tflite_path) / 1024:.2f} KB\")\n",
        "\n",
        "\n",
        "# --- 4. Quantization-Aware Training (QAT) ---\n",
        "\n",
        "print(\"\\n--- Applying Quantization-Aware Training (Demonstration with a simple model) ---\")\n",
        "# For QAT, we'll use a simple model and train it with QAT applied.\n",
        "qat_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Apply quantization-aware training wrappers\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_model(qat_model)\n",
        "quant_aware_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "quant_aware_model.summary()\n",
        "\n",
        "print(\"Training quantization-aware model (using dummy data)...\")\n",
        "# You would replace this with your actual training data and more epochs\n",
        "quant_aware_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=2, # Keep low for demonstration\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n",
        "# Convert QAT model to TFLite\n",
        "converter_qat = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter_qat.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_qat_model = converter_qat.convert()\n",
        "\n",
        "qat_tflite_path = os.path.join(OUTPUT_DIR, 'qat_tflite_model.tflite')\n",
        "with open(qat_tflite_path, 'wb') as f:\n",
        "    f.write(tflite_qat_model)\n",
        "\n",
        "qat_tflite_size = os.path.getsize(qat_tflite_path)\n",
        "print(f\"Quantization-Aware Trained TFLite model saved to: {qat_tflite_path}\")\n",
        "print(f\"Quantization-Aware Trained TFLite model size (raw): {qat_tflite_size / 1024:.2f} KB\")\n",
        "# print(f\"Quantization-Aware Trained TFLite model size (gzipped): {get_gzipped_model_size(qat_tflite_path) / 1024:.2f} KB\")\n",
        "\n",
        "print(\"\\n--- Summary of Model Sizes (Raw .tflite) ---\")\n",
        "print(f\"Baseline model (original .h5): {baseline_model_size / 1024:.2f} KB\")\n",
        "print(f\"Dynamic Range Quantized TFLite: {dr_model_size / 1024:.2f} KB\")\n",
        "if 'int_model_size' in locals():\n",
        "    print(f\"Full Integer Quantized TFLite: {int_model_size / 1024:.2f} KB\")\n",
        "print(f\"Pruned TFLite: {pruned_tflite_size / 1024:.2f} KB\")\n",
        "print(f\"Quantization-Aware Trained TFLite: {qat_tflite_size / 1024:.2f} KB\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading baseline model from: mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5\n",
            "Error loading baseline MobileNetV2 model: No file or directory found at mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224 (1).h5\n",
            "Creating a simple dummy model for further demonstrations.\n",
            "Dummy Baseline model size (raw): 15422.55 KB\n",
            "\n",
            "--- Applying Post-Training Dynamic Range Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Range Quantized model saved to: optimized_models/quantized_dynamic_range_model.tflite\n",
            "Dynamic Range Quantized model size (raw): 3856.03 KB\n",
            "\n",
            "--- Applying Post-Training Full Integer Quantization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Integer Quantized model saved to: optimized_models/quantized_full_integer_model.tflite\n",
            "Full Integer Quantized model size (raw): 3854.87 KB\n",
            "\n",
            "--- Applying Pruning (Demonstration with a simple model) ---\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d  (None, 222, 222, 32)      1762      \n",
            " _4 (PruneLowMagnitude)                                          \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 111, 111, 32)      1         \n",
            " oling2d_4 (PruneLowMagnitu                                      \n",
            " de)                                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 394272)            1         \n",
            " n_4 (PruneLowMagnitude)                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 10)                7885452   \n",
            " 4 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7887216 (30.09 MB)\n",
            "Trainable params: 3943626 (15.04 MB)\n",
            "Non-trainable params: 3943590 (15.04 MB)\n",
            "_________________________________________________________________\n",
            "Training pruned model (using dummy data)...\n",
            "Epoch 1/2\n",
            "4/4 [==============================] - 6s 898ms/step - loss: 54.5630 - accuracy: 0.0900 - val_loss: 101.5060 - val_accuracy: 0.1500\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 4s 864ms/step - loss: 92.1967 - accuracy: 0.1100 - val_loss: 97.5819 - val_accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Keras model saved to: optimized_models/pruned_keras_model.h5\n",
            "Pruned Keras model size (raw): 15421.12 KB\n",
            "Pruned TFLite model saved to: optimized_models/pruned_tflite_model.tflite\n",
            "Pruned TFLite model size (raw): 15407.12 KB\n",
            "\n",
            "--- Applying Quantization-Aware Training (Demonstration with a simple model) ---\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer_1 (Quantize  (None, 224, 224, 3)       3         \n",
            " Layer)                                                          \n",
            "                                                                 \n",
            " quant_conv2d_5 (QuantizeWr  (None, 222, 222, 32)      963       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_max_pooling2d_5 (Qua  (None, 111, 111, 32)      1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_flatten_5 (QuantizeW  (None, 394272)            1         \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_dense_5 (QuantizeWra  (None, 10)                3942735   \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3943703 (15.04 MB)\n",
            "Trainable params: 3943626 (15.04 MB)\n",
            "Non-trainable params: 77 (308.00 Byte)\n",
            "_________________________________________________________________\n",
            "Training quantization-aware model (using dummy data)...\n",
            "Epoch 1/2\n",
            "4/4 [==============================] - 5s 1s/step - loss: 7.1847 - accuracy: 0.1100 - val_loss: 10.1556 - val_accuracy: 0.0500\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 4s 1s/step - loss: 9.1793 - accuracy: 0.1200 - val_loss: 11.1462 - val_accuracy: 0.0500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization-Aware Trained TFLite model saved to: optimized_models/qat_tflite_model.tflite\n",
            "Quantization-Aware Trained TFLite model size (raw): 3855.23 KB\n",
            "\n",
            "--- Summary of Model Sizes (Raw .tflite) ---\n",
            "Baseline model (original .h5): 15422.55 KB\n",
            "Dynamic Range Quantized TFLite: 3856.03 KB\n",
            "Full Integer Quantized TFLite: 3854.87 KB\n",
            "Pruned TFLite: 15407.12 KB\n",
            "Quantization-Aware Trained TFLite: 3855.23 KB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}