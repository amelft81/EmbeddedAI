{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ/CMqktoj/Drkib0iCo76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/EmbeddedAI/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3x6GXHSNJgg",
        "outputId": "038ef2f8-f7eb-46c8-e9eb-4ff73c2695e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TensorFlow Benchmarking ---\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "\u001b[1m14536120/14536120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Model loaded successfully.\n",
            "Using dummy input data with shape: (1, 224, 224, 3)\n",
            "Performing a warm-up inference run...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Warm-up complete.\n",
            "\n",
            "--- Benchmark Results ---\n",
            "Total inferences: 100\n",
            "Total time taken: 15.97 seconds\n",
            "Average inference time: 159.67 ms\n",
            "Inferences per second (throughput): 6.26 FPS\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def benchmark_tensorflow_model():\n",
        "    \"\"\"\n",
        "    This function benchmarks a pre-trained MobileNetV2 model from tf.keras.applications.\n",
        "    It measures the average inference time and throughput (inferences per second).\n",
        "    \"\"\"\n",
        "    print(\"--- TensorFlow Benchmarking ---\")\n",
        "\n",
        "    # 1. Load a pre-trained model\n",
        "    # MobileNetV2 is a lightweight model suitable for performance testing.\n",
        "    # The first time you run this, it will download the model weights.\n",
        "    try:\n",
        "        model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model. Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Create dummy input data\n",
        "    # The input shape for MobileNetV2 is (224, 224, 3).\n",
        "    # We create a batch of 1 image with random data.\n",
        "    input_shape = (1, 224, 224, 3)\n",
        "    dummy_input = np.random.rand(*input_shape).astype(np.float32)\n",
        "    print(f\"Using dummy input data with shape: {input_shape}\")\n",
        "\n",
        "    # 3. Perform a warm-up run\n",
        "    # The first inference call can be slower due to model loading and graph initialization.\n",
        "    # This run ensures that subsequent timings are more representative of a steady state.\n",
        "    print(\"Performing a warm-up inference run...\")\n",
        "    _ = model.predict(dummy_input)\n",
        "    print(\"Warm-up complete.\")\n",
        "\n",
        "    # 4. Run the benchmark\n",
        "    num_inferences = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(num_inferences):\n",
        "        _ = model.predict(dummy_input, verbose=0) # verbose=0 silences the progress bar\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 5. Calculate and display results\n",
        "    total_time = end_time - start_time\n",
        "    avg_inference_time_ms = (total_time / num_inferences) * 1000\n",
        "    inferences_per_second = num_inferences / total_time\n",
        "\n",
        "    print(\"\\n--- Benchmark Results ---\")\n",
        "    print(f\"Total inferences: {num_inferences}\")\n",
        "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
        "    print(f\"Average inference time: {avg_inference_time_ms:.2f} ms\")\n",
        "    print(f\"Inferences per second (throughput): {inferences_per_second:.2f} FPS\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_tensorflow_model()\n",
        "\n"
      ]
    }
  ]
}