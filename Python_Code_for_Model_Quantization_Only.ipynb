{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amelft81/EmbeddedAI/blob/main/Python_Code_for_Model_Quantization_Only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Ensure this path correctly points to your 'simple_embedded_model.h5' file.\n",
        "# Based on your previous output, '/content/simple_embedded_model.h5' was the path used.\n",
        "SIMPLE_MODEL_PATH = '/content/simple_embedded_model.h5' # Adjust if your path is different\n",
        "\n",
        "# Output directory for optimized models\n",
        "OUTPUT_DIR = 'optimized_models'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define a representative dataset for full integer quantization\n",
        "# This should be a generator or a list of your actual input data.\n",
        "# It's crucial for calibrating the quantization ranges for activations.\n",
        "def representative_dataset_gen(model_input_shape, num_samples=100):\n",
        "    \"\"\"\n",
        "    A generator function for a representative dataset.\n",
        "    Generates dummy data based on the model's input shape.\n",
        "    REPLACE THIS WITH YOUR ACTUAL DATA LOADING LOGIC.\n",
        "    \"\"\"\n",
        "    for _ in range(num_samples):\n",
        "        # Generate random data matching the model's input shape (excluding batch dim)\n",
        "        data = np.random.rand(1, *model_input_shape).astype(np.float32)\n",
        "        yield [data]\n",
        "\n",
        "def quantize_model_only(model_path):\n",
        "    \"\"\"\n",
        "    Loads a Keras model and applies post-training dynamic range and full integer quantization,\n",
        "    saving the resulting TFLite models.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the original .h5 model file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting Quantization for {os.path.basename(model_path)} ---\")\n",
        "\n",
        "    # 1. Load the original model\n",
        "    model = None\n",
        "    try:\n",
        "        # Attempt to load the model directly\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Original model '{os.path.basename(model_path)}' loaded successfully.\")\n",
        "        model.summary()\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load model from '{model_path}'.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        print(\"Attempting to load with specific architecture for simple_embedded_model.h5...\")\n",
        "        try:\n",
        "            # Based on previous debugging, simple_embedded_model.h5 likely has InputLayer(shape=(10,))\n",
        "            # followed by Dense(8, relu) and Dense(1, sigmoid).\n",
        "            input_shape_for_dummy = (10,)\n",
        "            import re\n",
        "            match = re.search(r\"'batch_shape': \\[None, (\\d+)\\]\", str(e))\n",
        "            if match:\n",
        "                input_shape_for_dummy = (int(match.group(1)),)\n",
        "\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.InputLayer(input_shape=input_shape_for_dummy),\n",
        "                tf.keras.layers.Dense(8, activation='relu'),\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "            model.load_weights(model_path)\n",
        "            print(f\"Model architecture defined and weights loaded from '{os.path.basename(model_path)}'.\")\n",
        "            model.summary() # Show summary after successful loading\n",
        "        except Exception as load_weights_e:\n",
        "            print(f\"CRITICAL ERROR: Failed to load model even with specific architecture attempt: {load_weights_e}\")\n",
        "            print(\"Please provide the exact Keras architecture of your 'simple_embedded_model.h5' if this persists.\")\n",
        "            print(\"Exiting quantization process.\")\n",
        "            return\n",
        "\n",
        "    # Get the input shape for representative dataset generation\n",
        "    model_input_shape = model.input_shape[1:]\n",
        "\n",
        "    # --- 2. Apply Post-Training Dynamic Range Quantization ---\n",
        "    print(\"\\n--- Applying Post-Training Dynamic Range Quantization ---\")\n",
        "    converter_dr = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter_dr.optimizations = [tf.lite.Optimize.DEFAULT] # This enables dynamic range quantization by default\n",
        "\n",
        "    tflite_model_dr = converter_dr.convert()\n",
        "\n",
        "    dr_model_path = os.path.join(OUTPUT_DIR, 'quantized_dynamic_range_only_model.tflite')\n",
        "    with open(dr_model_path, 'wb') as f:\n",
        "        f.write(tflite_model_dr)\n",
        "\n",
        "    dr_model_size_kb = os.path.getsize(dr_model_path) / 1024\n",
        "    print(f\"Dynamic Range Quantized model saved to: {dr_model_path}\")\n",
        "    print(f\"Dynamic Range Quantized model size: {dr_model_size_kb:.2f} KB\")\n",
        "\n",
        "    # --- 3. Apply Post-Training Full Integer Quantization ---\n",
        "    print(\"\\n--- Applying Post-Training Full Integer Quantization ---\")\n",
        "    converter_int = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter_int.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter_int.representative_dataset = lambda: representative_dataset_gen(model_input_shape) # Pass input shape to generator\n",
        "\n",
        "    # Ensure all operations are quantized to integers. Fallback to float if not possible.\n",
        "    converter_int.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    # Set input/output types to float32 for inference, TFLite will handle conversion internally\n",
        "    converter_int.inference_input_type = tf.float32\n",
        "    converter_int.inference_output_type = tf.float32\n",
        "\n",
        "    try:\n",
        "        tflite_model_int = converter_int.convert()\n",
        "        int_model_path = os.path.join(OUTPUT_DIR, 'quantized_full_integer_only_model.tflite')\n",
        "        with open(int_model_path, 'wb') as f:\n",
        "            f.write(tflite_model_int)\n",
        "\n",
        "        int_model_size_kb = os.path.getsize(int_model_path) / 1024\n",
        "        print(f\"Full Integer Quantized model saved to: {int_model_path}\")\n",
        "        print(f\"Full Integer Quantized model size: {int_model_size_kb:.2f} KB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Full Integer Quantization failed. This might happen if the model operations are not fully supported for INT8 or representative dataset is insufficient. Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure you have TensorFlow installed:\n",
        "    # pip install tensorflow\n",
        "\n",
        "    # Call the function to quantize your simple_embedded_model.h5\n",
        "    quantize_model_only(SIMPLE_MODEL_PATH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Quantization for simple_embedded_model.h5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model 'simple_embedded_model.h5' loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m88\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m99\u001b[0m (400.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> (400.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m97\u001b[0m (388.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> (388.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Post-Training Dynamic Range Quantization ---\n",
            "Saved artifact at '/tmp/tmphwmhe4up'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139254315911312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315914576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315917264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315914192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Dynamic Range Quantized model saved to: optimized_models/quantized_dynamic_range_only_model.tflite\n",
            "Dynamic Range Quantized model size: 2.02 KB\n",
            "\n",
            "--- Applying Post-Training Full Integer Quantization ---\n",
            "Saved artifact at '/tmp/tmpwr2s71pt'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139254315911312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315914576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315917264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139254315914192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Integer Quantized model saved to: optimized_models/quantized_full_integer_only_model.tflite\n",
            "Full Integer Quantized model size: 2.58 KB\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "nOd5uW177cWo",
        "outputId": "f0bb2d07-fbdc-44ad-97f9-95c5785056e0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}